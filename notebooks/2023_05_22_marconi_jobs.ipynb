{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the energy profile of Marconi100 jobs\n",
    "\n",
    "In this notebook read the marconi data and i filter the jobs that have a processing time greater than 60 seconds and that have an energy profile (taking into account the p0_power metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_jobs = pd.read_parquet(\"../Marconi-test/plugin=job_table/metric=job_info_marconi100/a_0.parquet\")\n",
    "\n",
    "## Other power-related metrics available\n",
    "#\"../Marconi-test/plugin=ipmi_pub/metric=p1_power/a_0.parquet\"\n",
    "#\"../Marconi-test/plugin=ipmi_pub/metric=p0_io_power/a_0.parquet\"\n",
    "#\"../Marconi-test/plugin=ipmi_pub/metric=p1_io_power/a_0.parquet\"\n",
    "#\"../Marconi-test/plugin=ipmi_pub/metric=p0_mem_power/a_0.parquet\"\n",
    "#\"../Marconi-test/plugin=ipmi_pub/metric=p1_mem_power/a_0.parquet\"\n",
    "#\"../Marconi-test/plugin=ipmi_pub/metric=total_power/a_0.parquet\"\n",
    "df_power_p0_power = pd.read_parquet(\"../Marconi-test/plugin=ipmi_pub/metric=total_power/a_0.parquet\")\n",
    "#df_jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'value', 'node'], dtype='object')\n",
      "Index(['accrue_time', 'alloc_node', 'alloc_sid', 'array_job_id',\n",
      "       'array_max_tasks', 'array_task_id', 'array_task_str',\n",
      "       'array_task_throttle', 'assoc_id', 'batch_flag', 'batch_host',\n",
      "       'billable_tres', 'bitflags', 'boards_per_node', 'contiguous',\n",
      "       'cores_per_socket', 'cpus_alloc_layout', 'cpus_allocated',\n",
      "       'cpus_per_task', 'cpus_per_tres', 'dependency', 'derived_ec',\n",
      "       'eligible_time', 'end_time', 'exc_nodes', 'exit_code', 'features',\n",
      "       'group_id', 'job_id', 'job_state', 'last_sched_eval', 'max_cpus',\n",
      "       'max_nodes', 'mem_per_cpu', 'mem_per_node', 'min_memory_cpu',\n",
      "       'min_memory_node', 'nice', 'nodes', 'ntasks_per_board',\n",
      "       'ntasks_per_core', 'ntasks_per_core_str', 'ntasks_per_node',\n",
      "       'ntasks_per_socket', 'ntasks_per_socket_str', 'num_cpus', 'num_nodes',\n",
      "       'num_tasks', 'partition', 'pn_min_cpus', 'pn_min_memory',\n",
      "       'pn_min_tmp_disk', 'power_flags', 'priority', 'profile', 'qos',\n",
      "       'reboot', 'req_nodes', 'req_switch', 'requeue', 'resize_time',\n",
      "       'restart_cnt', 'resv_name', 'run_time', 'run_time_str', 'sched_nodes',\n",
      "       'shared', 'show_flags', 'sockets_per_board', 'sockets_per_node',\n",
      "       'start_time', 'state_reason', 'submit_time', 'suspend_time',\n",
      "       'threads_per_core', 'time_limit', 'time_limit_str', 'time_min',\n",
      "       'tres_alloc_str', 'tres_bind', 'tres_freq', 'tres_per_job',\n",
      "       'tres_per_node', 'tres_per_socket', 'tres_per_task', 'tres_req_str',\n",
      "       'user_id', 'wait4switch', 'wckey'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_power_p0_power['node'] = pd.to_numeric(df_power_p0_power['node'])\n",
    "df_power_p0_power['value'] = pd.to_numeric(df_power_p0_power['value'])\n",
    "print(df_power_p0_power.columns)\n",
    "print(df_jobs.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Converting timestamps to seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             3.0\n",
       "1         78983.0\n",
       "2             4.0\n",
       "3            12.0\n",
       "4             3.0\n",
       "           ...   \n",
       "239937    12918.0\n",
       "239938    78453.0\n",
       "239939     9166.0\n",
       "239940       13.0\n",
       "239941    78803.0\n",
       "Name: run_time, Length: 239942, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#df_jobs['run_time_seconds']\n",
    "\n",
    "df_jobs[\"run_time\"] = (df_jobs['end_time']-df_jobs['start_time']) / np.timedelta64(1, 's')\n",
    "df_jobs[\"run_time\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter 1: removing jobs that take less than a minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.0953563777913 % of the original job table\n"
     ]
    }
   ],
   "source": [
    "SECONDS_ONE_MINUTE = 60.0\n",
    "\n",
    "df_jobs_f1 = df_jobs.loc[df_jobs[\"run_time\"] >= SECONDS_ONE_MINUTE]\n",
    "perc_jobs_filtered = (len(df_jobs_f1)/len(df_jobs))*100\n",
    "print(str(perc_jobs_filtered), \"% of the original job table\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter 2: remove jobs with no energy profile\n",
    "\n",
    "Fast method for single-node jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.24846004451076 % of the original job table\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "#df_jobs_f12 = pd.DataFrame(lst_jobs_f12)\n",
    "\n",
    "#print(df_jobs_f12)\n",
    "\n",
    "## do faster for single node jobs\n",
    "df_jobs_f1_single = df_jobs_f1[df_jobs_f1[\"num_nodes\"]==1]\n",
    "\n",
    "#removing malformed \"nodes\"\n",
    "df_jobs_f1_single = df_jobs_f1_single[df_jobs_f1_single[\"nodes\"].str.match(\"\\[\\d+\\]\")].copy().reset_index(drop=True)\n",
    "df_jobs_f1_single[\"node\"] = [ast.literal_eval(x)[0] for x in df_jobs_f1_single['nodes']]\n",
    "\n",
    "\n",
    "#use this for smaller inputs\n",
    "#sample = df_jobs_f1_single[0:10000].copy().reset_index(drop=True)\n",
    "\n",
    "sample = df_jobs_f1_single\n",
    "\n",
    "group = df_power_p0_power.groupby(by=\"node\")\n",
    "#group.get_group(519)[\"timestamp\"]\n",
    "\n",
    "#result = sample.apply(lambda x: any(group.get_group(x[\"node\"])[\"timestamp\"].between(x[\"start_time\"], x[\"end_time\"])), axis=1)\n",
    "#print(result.values)\n",
    "#print(sample[[\"node\", \"start_time\", \"end_time\"]].values.tolist())\n",
    "\n",
    "\n",
    "#for debugging\n",
    "#result = [group.get_group(x[0])[\"timestamp\"].between(x[1], x[2]).value_counts() for x in sample[[\"node\", \"start_time\", \"end_time\"]].values.tolist()]\n",
    "\n",
    "#i know that 0 is node, 1 is start_time, and 2 is end_time\n",
    "result = [any(group.get_group(x[0])[\"timestamp\"].between(x[1], x[2])) for x in sample[[\"node\", \"start_time\", \"end_time\"]].values.tolist()]\n",
    "#print(result)\n",
    "sample[\"has_profile\"] = result\n",
    "df_jobs_f12_single = sample[sample[\"has_profile\"]==True]\n",
    "#[group.get_group(x[\"node\"][\"timestamp\"].between(x[\"start_time\"], x[\"end_time\"])) for x in sample.values.tolist()]\n",
    "\n",
    "#result = sample.any(df_power_p0_power[df_power_p0_power['node'] == node]\n",
    "#print(df_jobs_f1_single[\"node\"])\n",
    "\n",
    "#df_jobs_f1_single_2 = df_jobs_f1_single.apply(lambda x : x[re.search(\"\\[\\d+\\]\", x[\"nodes\"]) != None], axis=1)\n",
    "    \n",
    "    \n",
    "\n",
    "#df_jobs_f1_single[\"node\"] = ast.literal_eval(df_jobs_f1_single[\"nodes\"])[0]\n",
    "#print(df_jobs_f1_single.all(re.search(\"\\[\\d+\\]\", df_jobs_f1_single[\"nodes\"]) == None))\n",
    "\n",
    "\n",
    "perc_jobs_filtered = (len(df_jobs_f12_single)/len(df_jobs))*100\n",
    "print(str(perc_jobs_filtered), \"% of the original job table\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slower method for multi-node jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.77255349117982 % of the original job table\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "#df_jobs_f12 = pd.DataFrame(lst_jobs_f12)\n",
    "\n",
    "#print(df_jobs_f12)\n",
    "\n",
    "## do faster for single node jobs\n",
    "df_jobs_f1_multi = df_jobs_f1[df_jobs_f1[\"num_nodes\"] > 1].copy().reset_index(drop=True)\n",
    "\n",
    "#removing malformed \"nodes\"\n",
    "#df_jobs_f1_multi = df_jobs_f1_multi[df_jobs_f1_multi[\"nodes\"].str.match(\"\\[[^\\]+]\\]\")].copy().reset_index(drop=True)\n",
    "df_jobs_f1_multi[\"node\"] = [ast.literal_eval(x) for x in df_jobs_f1_multi['nodes']]\n",
    "#print(df_jobs_f1_multi)\n",
    "\n",
    "if True:\n",
    "    #use this for smaller inputs\n",
    "    #sample = df_jobs_f1_multi[0:10].copy().reset_index(drop=True)\n",
    "\n",
    "    sample = df_jobs_f1_multi\n",
    "\n",
    "    group = df_power_p0_power.groupby(by=\"node\")\n",
    "    #group.get_group(519)[\"timestamp\"]\n",
    "\n",
    "    #result = sample.apply(lambda x: any(group.get_group(x[\"node\"])[\"timestamp\"].between(x[\"start_time\"], x[\"end_time\"])), axis=1)\n",
    "    #print(result.values)\n",
    "    #print(sample[[\"node\", \"start_time\", \"end_time\"]].values.tolist())\n",
    "\n",
    "\n",
    "    #for debugging\n",
    "    #result = [group.get_group(x[0])[\"timestamp\"].between(x[1], x[2]).value_counts() for x in sample[[\"node\", \"start_time\", \"end_time\"]].values.tolist()]\n",
    "\n",
    "    #i know that 0 is node, 1 is start_time, and 2 is end_time\n",
    "    result = [all([any(group.get_group(y)[\"timestamp\"].between(x[1], x[2])) for y in x[0]]) for x in sample[[\"node\", \"start_time\", \"end_time\"]].values.tolist()]\n",
    "    \n",
    "    #result = [any(group.get_group(x[0])[\"timestamp\"].between(x[1], x[2])) for x in sample[[\"node\", \"start_time\", \"end_time\"]].values.tolist()]\n",
    "    #print(result)\n",
    "    \n",
    "    sample[\"has_profile\"] = result\n",
    "    df_jobs_f12_multi = sample[sample[\"has_profile\"]==True]\n",
    "   \n",
    "\n",
    "\n",
    "    perc_jobs_filtered = (len(df_jobs_f12_multi)/len(df_jobs_f1_multi))*100\n",
    "    print(str(perc_jobs_filtered), \"% of the original job table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "if False:\n",
    "    lst_jobs_f12_multi = []\n",
    "\n",
    "    df_jobs_f1_multi = df_jobs_f1[df_jobs_f1[\"num_nodes\"] > 1]\n",
    "\n",
    "    #print(len(df_jobs_f1_multi))\n",
    "\n",
    "    for index in range(len(df_jobs_f1_multi)): \n",
    "        #break   \n",
    "        job=df_jobs_f1_multi.iloc[index,:]\n",
    "        try:\n",
    "            nodes=ast.literal_eval(job['nodes'])\n",
    "        except ValueError:\n",
    "            #print(job['nodes'])\n",
    "            continue\n",
    "        start_time=job[\"start_time\"]\n",
    "        end_time=job[\"end_time\"]\n",
    "        for node in nodes:\n",
    "            #print(node)\n",
    "            df_node = df_power_p0_power.loc[df_power_p0_power['node'] == node]\n",
    "            df_node_job = df_node.loc[df_node['timestamp'].between(start_time, end_time)]\n",
    "            #df_node_job = df_power_p0_power[(df_power_p0_power.loc[df_power_p0_power['node'] == node]) & (df_power_p0_power['timestamp'].between(start_time, end_time))]\n",
    "\n",
    "            #np_timestamps = df_node['timestamp'].to_numpy()\n",
    "            #np_node_job = np_timestamps[(np_timestamps >= start_time) & (np_timestamps <= end_time)]        \n",
    "            \n",
    "            #print(df_node_job)\n",
    "            if not df_node_job.empty:\n",
    "                lst_jobs_f12_multi.append(job)\n",
    "                #print(df_node_job)\n",
    "        print(\"Job \"+str(index)+\" of \"+str(len(df_jobs_f1_multi)), end=\"\\r\")\n",
    "\n",
    "    df_jobs_f12_multi = pd.DataFrame(lst_jobs_f12_multi)\n",
    "\n",
    "    perc_jobs_filtered = (len(df_jobs_f12_multi)/len(df_jobs_f1_multi))*100\n",
    "    print(str(perc_jobs_filtered), \"% of the original job table\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_jobs_f12_single.to_csv(\"../Marconi-test/plugin=job_table/metric=job_info_marconi100/a_0_f12_singlenode.csv\", index=False)\n",
    "#df_jobs_f12_multi.to_csv(\"../Marconi-test/plugin=job_table/metric=job_info_marconi100/a_0_f12_multinode.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8893/1463414913.py:3: DtypeWarning: Columns (62,88) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_jobs_f12_viz = pd.read_csv(\"../Marconi-test/plugin=job_table/metric=job_info_marconi100/a_0_f12_singlenode.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_jobs_f12_viz = pd.read_csv(\"../Marconi-test/plugin=job_table/metric=job_info_marconi100/a_0_f12_singlenode.csv\")\n",
    "\n",
    "df_power_p0_power = pd.read_parquet(\"../Marconi-test/plugin=ipmi_pub/metric=total_power/a_0.parquet\")\n",
    "df_power_p0_power['node'] = pd.to_numeric(df_power_p0_power['node'])\n",
    "df_power_p0_power['value'] = pd.to_numeric(df_power_p0_power['value'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw jobs energy profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa625c2c81034aa5beb265c782ec98c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Save to PDF', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e18dc6748b54fe88d82a1c3477f0d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b4db25c17c4ad89d4cc261266d6f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=125365), Output()), _dom_classes=('widget-inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "#gfig = None\n",
    "\n",
    "def call_workflow(index):\n",
    "    get_job_energy_profile(index)\n",
    "\n",
    "def get_job_energy_profile(index):\n",
    "    global current_job_id, current_hostname\n",
    "    annot_str=''\n",
    "    job=df_jobs_f12_viz.iloc[index,:]\n",
    "    nodes=ast.literal_eval(job['nodes'])\n",
    "    start_time=job[\"start_time\"]\n",
    "    end_time=job[\"end_time\"]\n",
    "    for node in nodes:\n",
    "        print(node)\n",
    "        df_node = df_power_p0_power.loc[df_power_p0_power['node'] == node]\n",
    "        df_node_job = df_node.loc[df_node['timestamp'].between(start_time, end_time)]\n",
    "        if len(df_node_job) > 0:\n",
    "            print(df_node_job.describe(), end=\"\\r\")\n",
    "            plot_energy_profile(df_node_job, annot_str)\n",
    "    print(nodes)\n",
    "     \n",
    "    #plot_energy_profile(job_energy_profile, annot_str)\n",
    "\n",
    "    #current_job_id=str(job['job_id'])\n",
    "    #current_hostname=energy_host\n",
    "    ##\n",
    "    #describe=job_energy_profile.describe(percentiles=[.10, .25, .5, .75, .90])\n",
    "    #describe['job_id']=job['job_id']\n",
    "    #describe['socket']=socket\n",
    "    #describe['pp0']=describe[right_pp0]\n",
    "    #describe['DRAM']=describe[right_DRAM]\n",
    "    #describe['stat']=describe.index\n",
    "    #describe=describe.reset_index(drop=True)[arr_cols]  \n",
    "    #print(describe)  \n",
    "\n",
    "\n",
    "def plot_energy_profile(profile, annot_str):\n",
    "    TINY_SIZE = 2\n",
    "    SMALL_SIZE = 5\n",
    "    MEDIUM_SIZE = 25\n",
    "    BIGGER_SIZE = 50\n",
    "    FIG_WIDTH = 50\n",
    "    FIG_HEIGHT = 20\n",
    "\n",
    "    global gfig    \n",
    "\n",
    "    plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=BIGGER_SIZE)     # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=BIGGER_SIZE)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "    scatterplot_kwargs={'s': 50, 'palette': 'plasma'}\n",
    "    lineplot_kwargs={'linewidth': 5}\n",
    "\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(FIG_WIDTH,FIG_HEIGHT))\n",
    "    #ax = sns.boxplot(x='stat', y='value', data=plot_data, showfliers=False, hue='reading',\n",
    "    #             linewidth=TINY_SIZE)\n",
    "\n",
    "    ax = sns.scatterplot(data=profile, x='timestamp', y='value', **scatterplot_kwargs)\n",
    "    #ax = sns.lineplot(data=profile, x='timestamp', y='value', **lineplot_kwargs)\n",
    "\n",
    "    ## SET BORDERS SIZE AND WIDTH\n",
    "    [line.set_linewidth(TINY_SIZE) for line in ax.spines.values()]\n",
    "    [line.set_markersize(TINY_SIZE) for line in ax.yaxis.get_ticklines()]\n",
    "    [line.set_markeredgewidth(TINY_SIZE) for line in ax.yaxis.get_ticklines()]\n",
    "    [line.set_markersize(SMALL_SIZE) for line in ax.xaxis.get_ticklines()]\n",
    "    [line.set_markeredgewidth(TINY_SIZE) for line in ax.xaxis.get_ticklines()]\n",
    "    #ax.text(x=0.1,y=0.5,\n",
    "    #        s=annot_str,\n",
    "    #        fontdict=dict(color='red',size=MEDIUM_SIZE),\n",
    "    #        bbox=dict(facecolor='yellow',alpha=0.5),\n",
    "    #        horizontalalignment='left',\n",
    "    #        verticalalignment='center',\n",
    "    #        transform=ax.transAxes)\n",
    "    ax.set_ylabel('Processor Power (Watts)')\n",
    "    ax.set_xlabel('Timestamp')\n",
    "    gfig = fig\n",
    "\n",
    "button = widgets.Button(description=\"Save to PDF\")\n",
    "output = widgets.Output()\n",
    "\n",
    "display(button, output)\n",
    "\n",
    "## TODO: Pass on b the data (job_id, hostname, etc)\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        #fig=plt.gcf()\n",
    "        #print(gfig)\n",
    "        fig_filename='../Figures/marconi100_interact_plot_'+current_hostname+'_'+current_job_id+'.pdf'\n",
    "        gfig.savefig(fig_filename, format='pdf', dpi=300, bbox_inches='tight')\n",
    "        print('Plot saved as '+fig_filename)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "interact(call_workflow, index=widgets.IntSlider(min=0, max=len(df_jobs_f12_viz)-1, step=1, value=0));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- It seems that whatever is reading the p0 power has an imprecision of about two watts. We notice \"steps\" of two watts for many jobs. We may need to aggregate to have a smoother time series\n",
    "\n",
    "- Marconi100 information: https://wiki.u-gov.it/confluence/pages/viewpage.action?pageId=336727645\n",
    "  - It considers node sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
